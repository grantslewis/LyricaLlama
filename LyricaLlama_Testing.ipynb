{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e950842-def0-403e-8141-69c3388c9769",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-grantsl/.conda/envs/llm-gpu-quant/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, logging\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import gc         # garbage collect library\n",
    "\n",
    "\n",
    "# Check if a GPU is available and set the device accordingly\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)\n",
    "\n",
    "# Set up logging to display generation progress\n",
    "# logging.set_verbosity_info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2e9b4b0-65df-40fc-845f-76ada1d3e4eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\n",
    "You are a creative, world-famous expert lyricist. Write lyrics for a song, given just a title, artist name, possible genres, and any additional information provided.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def generate_input(song_name, artist, genres): #row):\n",
    "    # song_name = row['SName']\n",
    "    # artist = row['Artist']\n",
    "    # genres = row['Genres']\n",
    "    genre_info = \"\"\n",
    "    if genres != \"\" and genres is not None:\n",
    "        genre_info = ', '.join(genres)\n",
    "        genre_info = f\" using the following genres: {genre_info}\"\n",
    "\n",
    "    return f\"\"\"Write lyrics for a song titled \"{song_name}\" to be performed by {artist}{genre_info}.\"\"\" \n",
    "\n",
    "\n",
    "# def generate_text(inputs, system_prompt=DEFAULT_SYSTEM_PROMPT):\n",
    "#     return f\"\"\"### Instruction: {system_prompt}\n",
    "\n",
    "# ### Input:\n",
    "# {inputs}\n",
    "\n",
    "# ### Response:\n",
    "# \"\"\".strip() \n",
    "\n",
    "\n",
    "def generate_text(inputs, system_prompt=DEFAULT_SYSTEM_PROMPT):\n",
    "    return f\"\"\"{system_prompt} {inputs}\n",
    "\n",
    "### Lyrics:\n",
    "\"\"\"#.strip() \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def generate_response(question, tokenizer, model, max_length=2048, bad_words=['Bad Lyrics.', 'Not Safe For Work']): #, model_name=\"grantsl/LyricaLlama\"):\n",
    "    \"\"\"\n",
    "    Generates a response to the given question using the specified language model.\n",
    "    \"\"\"\n",
    "    bad_words_ids = None\n",
    "    if len(bad_words) > 0:\n",
    "        bad_words_ids = ' '.join(bad_words)\n",
    "        # Encode the negative prompt\n",
    "        bad_words_ids = tokenizer.encode(bad_words_ids, add_special_tokens=False)\n",
    "        bad_words_ids = [bad_words_ids]  # Wrap it in a list\n",
    "\n",
    "    # Encode the question and add the EOS token\n",
    "    input_ids = tokenizer.encode(question, return_tensors='pt').to(device)\n",
    "    \n",
    "    print('starting generation.')\n",
    "\n",
    "    # Generate a response\n",
    "    output = model.generate(input_ids, max_length=max_length, num_return_sequences=1, # early_stopping=True, \n",
    "                            no_repeat_ngram_size=2,\n",
    "                            length_penalty=1.0,\n",
    "                            bad_words_ids=bad_words_ids)\n",
    "    print('generation finished')\n",
    "\n",
    "    # Decode and return the response\n",
    "    return tokenizer.decode(output[0].to('cpu'), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d49446aa-5587-4694-835d-a127757edc5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.model from cache at /home/jupyter-grantsl/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852/tokenizer.model\n",
      "loading file tokenizer.json from cache at /home/jupyter-grantsl/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/jupyter-grantsl/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/jupyter-grantsl/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama-2-7b-hf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/jupyter-grantsl/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-2-7b-hf\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.32.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/jupyter-grantsl/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852/model.safetensors.index.json\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"transformers_version\": \"4.32.1\"\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.17it/s]\n",
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-7b-hf.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/jupyter-grantsl/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 4096,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9,\n",
      "  \"transformers_version\": \"4.32.1\"\n",
      "}\n",
      "\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting generation.\n",
      "generation finished\n",
      "starting generation.\n",
      "generation finished\n",
      "starting generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:21<01:46, 21.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation finished\n",
      "starting generation.\n",
      "generation finished\n",
      "starting generation.\n",
      "generation finished\n",
      "starting generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [01:57<04:20, 65.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation finished\n",
      "starting generation.\n",
      "generation finished\n",
      "starting generation.\n",
      "generation finished\n",
      "starting generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [03:06<03:21, 67.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation finished\n",
      "starting generation.\n",
      "generation finished\n",
      "starting generation.\n",
      "generation finished\n",
      "starting generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [04:18<02:17, 68.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation finished\n",
      "starting generation.\n",
      "generation finished\n",
      "starting generation.\n",
      "generation finished\n",
      "starting generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [05:08<01:02, 62.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation finished\n",
      "starting generation.\n",
      "generation finished\n",
      "starting generation.\n",
      "generation finished\n",
      "starting generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [06:39<00:00, 66.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation finished\n",
      "LyricaLlama\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "loading file tokenizer.model from cache at None\n",
      "loading file tokenizer.json from cache at /home/jupyter-grantsl/.cache/huggingface/hub/models--grantsl--LyricaLlama/snapshots/d96fa37b770ee629fa9fede3ce000a76ecfee2bb/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/jupyter-grantsl/.cache/huggingface/hub/models--grantsl--LyricaLlama/snapshots/d96fa37b770ee629fa9fede3ce000a76ecfee2bb/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/jupyter-grantsl/.cache/huggingface/hub/models--grantsl--LyricaLlama/snapshots/d96fa37b770ee629fa9fede3ce000a76ecfee2bb/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/jupyter-grantsl/.cache/huggingface/hub/models--grantsl--LyricaLlama/snapshots/d96fa37b770ee629fa9fede3ce000a76ecfee2bb/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"grantsl/LyricaLlama\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.32.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/jupyter-grantsl/.cache/huggingface/hub/models--grantsl--LyricaLlama/snapshots/d96fa37b770ee629fa9fede3ce000a76ecfee2bb/pytorch_model.bin.index.json\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"transformers_version\": \"4.32.1\"\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:18<00:00,  6.22s/it]\n",
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at grantsl/LyricaLlama.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/jupyter-grantsl/.cache/huggingface/hub/models--grantsl--LyricaLlama/snapshots/d96fa37b770ee629fa9fede3ce000a76ecfee2bb/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 4096,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9,\n",
      "  \"transformers_version\": \"4.32.1\"\n",
      "}\n",
      "\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting generation.\n",
      "generation finished\n",
      "starting generation.\n",
      "generation finished\n",
      "starting generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [01:37<08:06, 97.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation finished\n",
      "starting generation.\n",
      "generation finished\n",
      "starting generation.\n",
      "generation finished\n",
      "starting generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [03:13<06:26, 96.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation finished\n",
      "starting generation.\n",
      "generation finished\n",
      "starting generation.\n",
      "generation finished\n",
      "starting generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [04:48<04:47, 95.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation finished\n",
      "starting generation.\n",
      "generation finished\n",
      "starting generation.\n",
      "generation finished\n",
      "starting generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [06:26<03:13, 96.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation finished\n",
      "starting generation.\n",
      "generation finished\n",
      "starting generation.\n",
      "generation finished\n",
      "starting generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [08:03<01:36, 96.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation finished\n",
      "starting generation.\n",
      "generation finished\n",
      "starting generation.\n",
      "generation finished\n",
      "starting generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [09:39<00:00, 96.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model_name = \"grantsl/LyricaLlama\"\n",
    "models_to_test = [\"meta-llama/Llama-2-7b-hf\", \"grantsl/LyricaLlama\"]\n",
    "is_finetuned = [False, True]\n",
    "\n",
    "CURRENT_IND = 0\n",
    "\n",
    "RESP_COUNT = 3\n",
    "MAX_LEN = 500\n",
    "\n",
    "results = dict()\n",
    "\n",
    "for model_name, is_ft in zip(models_to_test, is_finetuned):\n",
    "# model_name = models_to_test[CURRENT_IND]\n",
    "\n",
    "\n",
    "    model_ver = os.path.basename(model_name)\n",
    "    print(model_ver)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "\n",
    "\n",
    "    test_set = [{'is_new':False, 'song':'All I Want For Christmas', 'artist':'Mariah Carey', 'genres':['Christmas']},\n",
    "                {'is_new':False, 'song':'Blinding Lights', 'artist':'The Weeknd', 'genres':['Pop', 'Synth-Pop']},\n",
    "                {'is_new':False, 'song':'Lover', 'artist':'Taylor Swift', 'genres':['Pop']},\n",
    "                {'is_new':True, 'song':'Electric Touch', 'artist':'Taylor Swift and Fall Out Boy', 'genres':['Pop', 'Pop/Rock']},\n",
    "                {'is_new':True, 'song':'Next Thing You Know', 'artist':'Jordan Davis', 'genres':['Country']},\n",
    "                {'is_new':True, 'song':'Penthouse', 'artist':'Kelsea Ballerini', 'genres':['Pop', 'Country/Pop']},\n",
    "               ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for song_info in tqdm(test_set):\n",
    "        res_song = song_info.copy()\n",
    "        # print(song_info.keys())\n",
    "        song = song_info['song']\n",
    "        artist = song_info['artist']\n",
    "        genres = song_info['genres']\n",
    "        \n",
    "        \n",
    "        if song in results:\n",
    "            res_song = results[song]\n",
    "        else:\n",
    "            # res_song['prompt'] = []\n",
    "            res_song['results'] = []\n",
    "            # res_song['is_finetuned'] = []\n",
    "\n",
    "        inputs = generate_input(song, artist, genres)\n",
    "        inputs = generate_text(inputs)\n",
    "\n",
    "        inputs_len = len(inputs)\n",
    "\n",
    "        # responses = []\n",
    "        for i in range(RESP_COUNT):\n",
    "            res_line = dict()\n",
    "            \n",
    "            # res_song['prompt'].append(inputs)\n",
    "            res_line['model_name'] = model_name\n",
    "            res_line['prompt'] = inputs\n",
    "            response = generate_response(inputs, tokenizer, model, min(MAX_LEN + inputs_len, 4096))\n",
    "\n",
    "            response_cleaned = response[inputs_len:]\n",
    "            # responses.append(response_cleaned)\n",
    "            # res_song['responses'].append(responses_cleaned)\n",
    "            # res_song['is_finetuned'].append(is_ft)\n",
    "            res_line['responses'] = response_cleaned\n",
    "            res_line['is_finetuned'] = is_ft\n",
    "            \n",
    "            res_song['results'].append(res_line)\n",
    "\n",
    "        # res_song['responses'] = responses\n",
    "\n",
    "        results[song] = res_song\n",
    "\n",
    "    # with open(f'./results_{model_ver}.json', 'w') as f:\n",
    "    #     json.dump(results, f, indent=4)\n",
    "\n",
    "    del model\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache() \n",
    "\n",
    "with open(f'./combined_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e35c8a-7b43-43be-af27-5d6160b1489c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = generate_input('All I Want For Christmas', 'Mariah Carey', ['Christmas'])\n",
    "inputs = generate_input('Song_Title', 'Artist_Name', ['Genre(s)'])\n",
    "\n",
    "\n",
    "question = generate_text(inputs)\n",
    "\n",
    "print('Prompt:', question) \n",
    "# Generate and print the response\n",
    "response1 = generate_response(question, tokenizer, model, max_length=500)\n",
    "print('Results:\\n', response1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-gpu-quant",
   "language": "python",
   "name": "llm-gpu-quant"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
